{
 "metadata": {
  "name": "assignment1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Assignment 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "styles = open(\"../Style.css\").read()\n",
      "HTML(styles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    div.cell{\n",
        "        width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    div.text_cell_render{\n",
        "        line-height: 145%;\n",
        "        width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "        font-family: \"Source Code Pro\", source-code-pro, Consolas, monospace;\n",
        "    }\n",
        "\n",
        "</style>\n"
       ],
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x1047f4f10>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import json\n",
      "\n",
      "pages = 1\n",
      "\n",
      "for i in xrange(1, pages + 1):\n",
      "    response = urllib.urlopen('http://search.twitter.com/search.json?q=microsoft&page=' + str(i))\n",
      "    results = json.load(response)['results']\n",
      "\n",
      "    for res in results:\n",
      "        print res['text']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Walk to Microsoft/Google; Move-in Bonus! Spacious mfd home #165 (mountain view) $2695 3bd: **Status: PENDING**... http://t.co/nBGIttLljw\n",
        "Microsoft integra Skype dentro http://t.co/eAYx0Vjlwl http://t.co/cbQ5Oyy8ma\n",
        "Microsoft: Don't expect a Windows 8 iTunes app soon:  http://t.co/EJWMqUk8Hl #Technology\n",
        "VR-Zone | Technology News | Products Reviews http://t.co/z1oxZtVAV5 v\u00eda @VRZone\n",
        "RT @MicrosoftAjuda: Voc\u00ea tem perguntas sobre sua #ContaMicrosoft (http://t.co/jvDMQPKevb)? Fa\u00e7a sua pergunta no nosso f\u00f3rum! http://t.co/hr3tPEsDZL\n",
        "RT @whymicrosoft: Microsoft to grow #Office365\u2019s capacity to import contacts from 3rd party tools: http://t.co/zdUNv6naBE\n",
        "@EastonRoyce Microsoft releases new YouTube app for Windows Phone 8 http://t.co/QUlkZjFZQa\n",
        "RT @dens: Oh hello there, sexy new @Foursquare app for Windows Phone 8!  http://t.co/WrZTR6vuqQ \u2026  (ps:  pre-installed on new Nokia Lumias :)\n",
        "@EastonRoyce Microsoft returns to Gamescom in August 2013 for next Xbox http://t.co/PAcoLfdNVy\n",
        "Antes da Surface a Microsoft falava que o ruim do iPad era que ele n\u00e3o consegui produzir conte\u00fado e agora \u00e9 a falta de teclado f\u00edsico hahah\n",
        "BTW @nettechjax has a wonderful Helpdesk. @pcmag just called the Surface @Microsoft top products of 2013.\n",
        "@EastonRoyce Microsoft releases new YouTube app for Windows Phone 8 http://t.co/6CUsP3gy01\n",
        "The intergalactic bounty hunter Lobo is now available on the Xbox Live Marketplace for 400 Microsoft Points or... http://t.co/oVB98I6dho\n",
        "@EastonRoyce Microsoft returns to Gamescom in August 2013 for next Xbox http://t.co/l8yMgLACNv\n",
        "NEW MICROSOFT EXCEL STICKER FOR KEYBOARD: The MicrosoftTM Excel keyboard stickers can easily transform your st... http://t.co/54VNORl3Ak\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python twitterstream.py > output.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^CTraceback (most recent call last):\r\n",
        "  File \"twitterstream.py\", line 62, in <module>\r\n",
        "    fetchsamples()\r\n",
        "  File \"twitterstream.py\", line 57, in fetchsamples\r\n",
        "    for line in response:\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 530, in next\r\n",
        "    line = self.readline()\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 447, in readline\r\n",
        "    data = self._sock.recv(self._rbufsize)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/httplib.py\", line 567, in read\r\n",
        "    s = self.fp.read(amt)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 380, in read\r\n",
        "    data = self._sock.recv(left)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/ssl.py\", line 241, in recv\r\n",
        "    return self.read(buflen)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/ssl.py\", line 160, in read\r\n",
        "    return self._sslobj.read(len)\r\n",
        "KeyboardInterrupt\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# need 20 lines\n",
      "!head -n1 output.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"delete\":{\"status\":{\"id\":331106645725806592,\"user_id\":484704907,\"id_str\":\"331106645725806592\",\"user_id_str\":\"484704907\"}}}\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head AFINN-111.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abandon\t-2\r\n",
        "abandoned\t-2\r\n",
        "abandons\t-2\r\n",
        "abducted\t-2\r\n",
        "abduction\t-2\r\n",
        "abductions\t-2\r\n",
        "abhor\t-3\r\n",
        "abhorred\t-3\r\n",
        "abhorrent\t-3\r\n",
        "abhors\t-3\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build word:sentiment dictionary\n",
      "word = []\n",
      "score = []\n",
      "with open('AFINN-111.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        a, b = line.split('\\t')\n",
      "        word.append(unicode(a, 'UTF-8'))\n",
      "        score.append(int(b))\n",
      "    sentiment = dict(zip(word, score))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to do some characterisations of the dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = [x.split() for x in sentiment]\n",
      "keylen = np.array([len(x) for x in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some entries consist of more than one words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array(words)[keylen > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([[u'cool', u'stuff'], [u'green', u'wash'], [u'messing', u'up'],\n",
        "       [u'does', u'not', u'work'], [u\"can't\", u'stand'], [u'no', u'fun'],\n",
        "       [u'not', u'working'], [u'right', u'direction'], [u'some', u'kind'],\n",
        "       [u'cashing', u'in'], [u'screwed', u'up'], [u'dont', u'like'],\n",
        "       [u'green', u'washing'], [u'fed', u'up'], [u'not', u'good']], dtype=object)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This complicates the matter. If a tweet contains the phrase \"...cool stuff...\", how should the sentiment be computed? based on \"cool\" only, \"cool stuff\" only, or the sum of both \"cool\" and \"cool stuff\"? \n",
      "\n",
      "The given definition of **tweet sentiment** is: **the sum of the sentiment scores for each term in the tweet**. The definition doesn't say anything on what to do if a tweet contains n-grams ($n \\geq 1$) which are all in the dictionary.\n",
      "\n",
      "The maximum phrase length in the dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max(keylen)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does any of the dictionary words/phrases contain some punctuation marks?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 in [c in x for x in sentiment for c in '.,?!']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "\n",
      "string.punctuation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The actual computation of the tweet sentiment:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the mapping is used for unicode `translate` method\n",
      "remove_punctuation_map = dict((ord(char), None) for char in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: lower case, no punctuation\n",
      "            words = json.loads(line)['text'].translate(remove_punctuation_map).lower().strip().split()\n",
      "            \n",
      "            # chosen n-gram strategy: build all n-grams and consider all matches\n",
      "            # we know that there's only up to 3 words in the dict\n",
      "            temp = list(words)\n",
      "            for n in xrange(2, 4):\n",
      "                myNgrams = [temp[i:i+n] for i in xrange(len(temp) - n)]\n",
      "                for x in myNgrams:\n",
      "                    words.append(' '.join(x))\n",
      "\n",
      "            print float( sum([sentiment[x] if x in sentiment else 0 for x in words]) )\n",
      "            \n",
      "        except:\n",
      "            pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "6.0\n",
        "-3.0\n",
        "0.0\n",
        "0.0\n",
        "4.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "-2.0\n",
        "0.0\n",
        "3.0\n",
        "-3.0\n",
        "2.0\n",
        "-4.0\n",
        "0.0\n",
        "2.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem specification is not so clear, i.e. not specific, about how exactly to assign score to unknown words. A simple approach is just to assign a score to every unknown word in a tweet equal to the sentiment of the tweet. \n",
      "\n",
      "There might be repetitions of one word, with different assigned sentiment, because if a word appears in a tweet and given a sentiment value according to the tweet, it may appear again sometime later in a different tweet which has a different sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the mapping is used for unicode `translate` method\n",
      "remove_punctuation_map = dict((ord(char), None) for char in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: lower case, no punctuation\n",
      "            # we don't consider n-grams here, only one-words\n",
      "            words = json.loads(line)['text'].translate(remove_punctuation_map).lower().strip().split()\n",
      "\n",
      "            score = 0\n",
      "            unk = []\n",
      "            for x in words:\n",
      "                if x in sentiment:\n",
      "                    score += sentiment[x]\n",
      "                else:\n",
      "                    unk.append(x)\n",
      "            \n",
      "            # a very simplistic approach: assign unknown word a score equal to its tweet\n",
      "            for x in unk:\n",
      "                print '%s %.3f' % (x, score)\n",
      "            \n",
      "        except:\n",
      "            pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "rt 0.000\n",
        "augustolorenzz 0.000\n",
        "a 0.000\n",
        "primeira 0.000\n",
        "do 0.000\n",
        "ano 0.000\n",
        "ja 0.000\n",
        "foi 0.000\n",
        "rt 6.000\n",
        "maryummalik 6.000\n",
        "one 6.000\n",
        "direction's 6.000\n",
        "number 6.000\n",
        "1 6.000\n",
        "zayn's 6.000\n",
        "grandma 6.000\n",
        "2famalik 6.000\n",
        "doniyaelisha 6.000\n",
        "aaroosam 6.000\n",
        "wamalikk98 6.000\n",
        "fizzahx 6.000\n",
        "httptco\u2026 6.000\n",
        "rt -3.000\n",
        "meatsauce1 -3.000\n",
        "f -3.000\n",
        "word -3.000\n",
        "parise -3.000\n",
        "shreds -3.000\n",
        "f -3.000\n",
        "you -3.000\n",
        "hawks -3.000\n",
        "rt 0.000\n",
        "gooodapollo 0.000\n",
        "global 0.000\n",
        "and 0.000\n",
        "warped 0.000\n",
        "tour 0.000\n",
        "are 0.000\n",
        "really 0.000\n",
        "gay 0.000\n",
        "\ud83d\ude0f\ud83d\udc4d 0.000\n",
        "rt 0.000\n",
        "latingable1d 0.000\n",
        "arlenysluna 0.000\n",
        "hi 0.000\n",
        "how 4.000\n",
        "you 4.000\n",
        "fell 4.000\n",
        "asleep 4.000\n",
        "when 4.000\n",
        "you 4.000\n",
        "read 4.000\n",
        "my 4.000\n",
        "message 4.000\n",
        "cara 0.000\n",
        "httptcowilzgkwvsw 0.000\n",
        "ajjenkins 0.000\n",
        "sup 0.000\n",
        "food 0.000\n",
        "baby 0.000\n",
        "rt 0.000\n",
        "drsoxion 0.000\n",
        "es 0.000\n",
        "incre\u00edble 0.000\n",
        "la 0.000\n",
        "evoluci\u00f3n 0.000\n",
        "de 0.000\n",
        "personajes 0.000\n",
        "desde 0.000\n",
        "iron 0.000\n",
        "man 0.000\n",
        "1 0.000\n",
        "hasta 0.000\n",
        "la 0.000\n",
        "3 0.000\n",
        "aee 0.000\n",
        "comemora 0.000\n",
        "com 0.000\n",
        "n\u00f3is 0.000\n",
        "porque 0.000\n",
        "se 0.000\n",
        "depender 0.000\n",
        "do 0.000\n",
        "timinho 0.000\n",
        "de 0.000\n",
        "voc\u00eas 0.000\n",
        "kkkkkkkk' 0.000\n",
        "fatty -2.000\n",
        "14saik14 0.000\n",
        "' 0.000\n",
        "rt 3.000\n",
        "myidolovesme 3.000\n",
        "l'evoluzione 3.000\n",
        "dei 3.000\n",
        "dm 3.000\n",
        "all'inizio 3.000\n",
        "did 3.000\n",
        "you 3.000\n",
        "this 3.000\n",
        "pic 3.000\n",
        "of 3.000\n",
        "you 3.000\n",
        "dopo 3.000\n",
        "sei 3.000\n",
        "la 3.000\n",
        "perfezione 3.000\n",
        "ora 3.000\n",
        "ti 3.000\n",
        "puzza 3.000\n",
        "il 3.000\n",
        "culo 3.000\n",
        "let 3.000\n",
        "m\u2026 3.000\n",
        "20 -3.000\n",
        "minutes -3.000\n",
        "in -3.000\n",
        "and -3.000\n",
        "door -3.000\n",
        "has -3.000\n",
        "slowly -3.000\n",
        "opened -3.000\n",
        "by -3.000\n",
        "itself -3.000\n",
        "paranormalactivity4 -3.000\n",
        "andyf2k10 2.000\n",
        "everyone 2.000\n",
        "i 2.000\n",
        "talk 2.000\n",
        "to 2.000\n",
        "seems 2.000\n",
        "to 2.000\n",
        "be 2.000\n",
        "hooked 2.000\n",
        "on 2.000\n",
        "it 2.000\n",
        "it 2.000\n",
        "must 2.000\n",
        "be 2.000\n",
        "it 2.000\n",
        "yo -4.000\n",
        "mama -4.000\n",
        "is -4.000\n",
        "so -4.000\n",
        "fat -4.000\n",
        "her -4.000\n",
        "has -4.000\n",
        "it's -4.000\n",
        "own -4.000\n",
        "congressman -4.000\n",
        "mi 0.000\n",
        "lasagna 0.000\n",
        "comida 0.000\n",
        "italia 0.000\n",
        "likeforlike 0.000\n",
        "httptco5u6rca5aqw 0.000\n",
        "mi 2.000\n",
        "churri 2.000\n",
        "claraaa 2.000\n",
        "es 2.000\n",
        "una 2.000\n",
        "maquina 2.000\n",
        "que 2.000\n",
        "me 2.000\n",
        "mejorado 2.000\n",
        "mucho 2.000\n",
        "la 2.000\n",
        "noche 2.000\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordict = {}\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: don't consider n-grams\n",
      "            words = json.loads(line)['text'].strip().split()\n",
      "\n",
      "            for x in words:\n",
      "                if x in wordict:\n",
      "                    wordict[x] += 1\n",
      "                else:\n",
      "                    wordict[x] = 1\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "\n",
      "denom = sum(wordict.values())\n",
      "\n",
      "for k, v in wordict.iteritems():\n",
      "    print '%s %.4f' % (k.encode('UTF-8'), float(v) / denom)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":) 0.0054\n",
        "L'EVOLUZIONE 0.0054\n",
        "evoluci\u00f3n 0.0054\n",
        "fat, 0.0054\n",
        "asleep\" 0.0054\n",
        "RT 0.0380\n",
        "1 0.0109\n",
        "ha 0.0054\n",
        "20 0.0054\n",
        "how 0.0054\n",
        "seems 0.0054\n",
        ", 0.0054\n",
        "to 0.0109\n",
        "culo\" 0.0054\n",
        "depender 0.0054\n",
        "has 0.0109\n",
        "worth 0.0054\n",
        "@augustolorenzz: 0.0054\n",
        "porque 0.0054\n",
        "do 0.0109\n",
        "it, 0.0054\n",
        "read 0.0054\n",
        "de 0.0109\n",
        "YEAH 0.0054\n",
        "#italia 0.0054\n",
        "it! 0.0054\n",
        "SHREDS. 0.0054\n",
        "@AndyF2K10 0.0054\n",
        "maquina 0.0054\n",
        "Global 0.0054\n",
        "\"did 0.0054\n",
        "foi 0.0054\n",
        ":'( 0.0054\n",
        "desde 0.0054\n",
        "kkkkkkkk' 0.0054\n",
        "@Latingable1D: 0.0054\n",
        "COMEMORA 0.0054\n",
        "ass 0.0054\n",
        "everyone 0.0054\n",
        "@myidolovesme: 0.0054\n",
        "HAWKS 0.0054\n",
        "DEI 0.0054\n",
        "are 0.0054\n",
        "Mama 0.0054\n",
        "HELL 0.0054\n",
        "es 0.0054\n",
        "really 0.0054\n",
        "# 0.0054\n",
        "opened 0.0054\n",
        "COM 0.0054\n",
        "hooked 0.0054\n",
        "#comida 0.0054\n",
        "noche 0.0054\n",
        "@wamalikk98 0.0054\n",
        "@__claraaa 0.0054\n",
        "WORD!!! 0.0054\n",
        "be 0.0109\n",
        "perfezione.\" 0.0054\n",
        "incre\u00edble 0.0054\n",
        "Mi 0.0054\n",
        "m\u2026 0.0054\n",
        "http://t.co/WILzGKWvsw 0.0054\n",
        "cara 0.0054\n",
        "ja 0.0054\n",
        "lol\" 0.0054\n",
        "http://t.co/\u2026 0.0054\n",
        "mejorado 0.0054\n",
        "#ParanormalActivity4 0.0054\n",
        "miss 0.0054\n",
        "of 0.0054\n",
        "la 0.0217\n",
        "DM. 0.0054\n",
        "una 0.0054\n",
        "tour 0.0054\n",
        "3. 0.0054\n",
        "let 0.0054\n",
        "@DrSoxion: 0.0054\n",
        "personajes 0.0054\n",
        "Es 0.0054\n",
        "own 0.0054\n",
        "itself. 0.0054\n",
        "number 0.0054\n",
        "Lol 0.0054\n",
        "mucho 0.0054\n",
        "warped 0.0054\n",
        "@DoniyaElisha 0.0054\n",
        "message 0.0054\n",
        "@Aaroosa_M 0.0054\n",
        "http://t.co/5u6rcA5aQW 0.0054\n",
        "\"fell 0.0054\n",
        "\"sei 0.0054\n",
        "gay. 0.0054\n",
        "her 0.0054\n",
        "sup? 0.0054\n",
        "it's 0.0054\n",
        "by 0.0054\n",
        "Grandma) 0.0054\n",
        "Disappointed. 0.0054\n",
        "slowly 0.0054\n",
        "baby. 0.0054\n",
        "pic 0.0054\n",
        "door 0.0054\n",
        "F 0.0109\n",
        "you? 0.0054\n",
        "hi 0.0054\n",
        "Aee 0.0054\n",
        "timinho 0.0054\n",
        "\"ti 0.0054\n",
        "must 0.0054\n",
        "me 0.0054\n",
        "this 0.0054\n",
        "churri 0.0054\n",
        "mi 0.0054\n",
        "congressman. 0.0054\n",
        "\ud83d\ude0f\ud83d\udc4d 0.0054\n",
        "my 0.0054\n",
        "and 0.0109\n",
        "puzza 0.0054\n",
        "minutes 0.0054\n",
        "is 0.0054\n",
        "ano 0.0054\n",
        "it 0.0054\n",
        "all'inizio: 0.0054\n",
        "il 0.0054\n",
        "in 0.0054\n",
        "@Fizzahx 0.0054\n",
        "ora: 0.0054\n",
        "@gooodapollo: 0.0054\n",
        "funny 0.0054\n",
        "Yo 0.0054\n",
        "no 0.0054\n",
        "Food 0.0054\n",
        "fan.. 0.0054\n",
        "when 0.0054\n",
        "#likeforlike 0.0054\n",
        "@Arlenysluna 0.0054\n",
        "N\u00d3IS 0.0054\n",
        "que 0.0054\n",
        "you 0.0163\n",
        "@ajjenkins 0.0054\n",
        "Man 0.0054\n",
        ":):) 0.0054\n",
        "voc\u00eas 0.0054\n",
        "fatty 0.0054\n",
        "I 0.0054\n",
        "primeira 0.0054\n",
        "Direction's 0.0054\n",
        "@2fa_Malik 0.0054\n",
        "@MeatSauce1: 0.0054\n",
        "Iron 0.0054\n",
        "YOU 0.0054\n",
        "on 0.0054\n",
        "@14Saik14 0.0054\n",
        "a 0.0054\n",
        "PARISE 0.0054\n",
        "#lasagna 0.0054\n",
        "One 0.0054\n",
        "hasta 0.0054\n",
        "@MaryumMalik_: 0.0054\n",
        "dopo: 0.0054\n",
        "so 0.0054\n",
        "talk 0.0054\n",
        ",, 0.0054\n",
        "se 0.0054\n",
        "(Zayn's 0.0054\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the mapping is used for unicode `translate` method\n",
      "remove_punctuation_map = dict((ord(char), None) for char in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
      "\n",
      "# data structure: state : [count, sum]\n",
      "states = {x : [0, 0] for x in [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
      "                               \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
      "                               \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
      "                               \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
      "                               \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]}\n",
      "\n",
      "with open('output_20130505_2351.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "            \n",
      "            country = tweet['place']['country']\n",
      "            state = tweet['place']['full_name'][-2::]\n",
      "            \n",
      "            if country=='United States' and state in states:\n",
      "                # chosen normalisation: lower case, no punctuation, don't consider n-grams\n",
      "                words = tweet['text'].translate(remove_punctuation_map).lower().strip().split()\n",
      "                sent = float( sum([sentiment[x] if x in sentiment else 0 for x in words]) )\n",
      "            \n",
      "                states[state][0] += 1\n",
      "                states[state][1] += sent\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "\n",
      "# compute the value that should be used for ranking\n",
      "def get_rank(v):\n",
      "    if states[v][0]:\n",
      "        return float(states[v][1]) / states[v][0]\n",
      "    else:\n",
      "        # if there's no tweet from the state, give large negative\n",
      "        return -1000\n",
      "    \n",
      "print str(max(states, key=get_rank)).encode('UTF-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NV\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordict = {}\n",
      "\n",
      "with open('output_20130505_2351.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "            \n",
      "            for ht in tweet['entities']['hashtags']:\n",
      "                word = ht['text']\n",
      "                \n",
      "                if word in wordict:\n",
      "                    wordict[word] += 1\n",
      "                else:\n",
      "                    wordict[word] = 1\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "        \n",
      "top_ten = sorted(wordict, key=lambda x: wordict[x], reverse=True)[:10]\n",
      "\n",
      "for i in top_ten:\n",
      "    print i.encode('UTF-8'), float(wordict[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT 19.0\n",
        "gameinsight 15.0\n",
        "LifeWouldBeAlotBetterIf 8.0\n",
        "Nicol\u00e1sT\u00faNoGanaste 7.0\n",
        "ipad 7.0\n",
        "A2MesesDeTuPartidaChavez 7.0\n",
        "ipadgames 7.0\n",
        "QueCadaCaprilistaTengaUnMillonDeSeguidores 7.0\n",
        "androidgames 6.0\n",
        "veracruz 6.0\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}