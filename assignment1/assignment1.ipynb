{
 "metadata": {
  "name": "assignment1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Assignment 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "styles = open(\"../Style.css\").read()\n",
      "HTML(styles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    div.cell{\n",
        "        width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    div.text_cell_render{\n",
        "        line-height: 145%;\n",
        "        width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "        font-family: \"Source Code Pro\", source-code-pro, Consolas, monospace;\n",
        "    }\n",
        "\n",
        "</style>\n"
       ],
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x104924f10>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import urllib\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pages = 1\n",
      "\n",
      "for i in xrange(1, pages + 1):\n",
      "    response = urllib.urlopen('http://search.twitter.com/search.json?q=microsoft&page=' + str(i))\n",
      "    results = json.load(response)['results']\n",
      "\n",
      "    for res in results:\n",
      "        print res['text']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Microsoft Excel 2010 Course Beginners/ Intermediate Training #msexcel http://t.co/7Ph7wEF2OD\n",
        "@077707 T'utilises un ordinateur ou un t\u00e9l\u00e9phone pour tweeter ? Enfin dans les 2 il y a des composants fait en Isra\u00ebl... Intel, Microsoft,..\n",
        "RT @elyas360: RT y s\u00edgueme si quieres participar en el sorteo de 800 Microsoft Points\n",
        "Microsoft \u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0430 \u0437\u0430\u043f\u043b\u0430\u0442\u0438\u0442\u044c 1 \u043c\u043b\u0440\u0434 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 \u0437\u0430 \u0431\u0440\u0435\u043d\u0434 Nook http://t.co/QrvmGrJdiW #FF_RU\n",
        "Now Pete Rose Is Helping Microsoft Scare People Away From Google Apps (GOOG, MSFT) - http://t.co/57665cb0zo\n",
        "@Aine_Cookie jajajaja si tienes virus lo mejor es Microsoft Security Essentials. Facil rapido y sobre todo, gratis jaja\n",
        "Microsoft To Get A Major Revenue Growth Boost From Cloud Computing - Seeking Alpha http://t.co/5721Q9IXhY\n",
        "Microsoft i FB u zajedni\u010dkoj borbi protiv Google-a http://t.co/PgDo0XTddg\n",
        "Microsoft's Asian Windows 8 Ads Are Relatively Insane - Forbes http://t.co/gBUz7zO0mO\n",
        "Microsoft details Patch Tuesday Internet Explorer fixes http://t.co/VRZfS1bNDg\n",
        "Microsoft can keep playing as much commercial dubstep as they want in their adverts, they will never ever make Internet Explorer cool...\n",
        "Watch Microsoft's new Google Docs bashing ads feat. Rob Schnieder. Weird. http://t.co/R7eUjpvlcP\n",
        "@xxxIceCreamPie ahahaha. Hwaiting! I'm DLing microsoft office to le phone to open the attachments \u2661\n",
        "De los grandes emprendedores del siglo XX @afontanini destaca a @BillGates, pionero creando Microsoft, visionario y emprendedor social\n",
        "De los grandes emprendedores del siglo XX @afontanini destaca a @BillGates, pionero creando Microsoft, visionario y emprendedor social\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 1: collect tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python twitterstream.py > output.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^CTraceback (most recent call last):\r\n",
        "  File \"twitterstream.py\", line 62, in <module>\r\n",
        "    fetchsamples()\r\n",
        "  File \"twitterstream.py\", line 57, in fetchsamples\r\n",
        "    for line in response:\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 530, in next\r\n",
        "    line = self.readline()\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 447, in readline\r\n",
        "    data = self._sock.recv(self._rbufsize)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/httplib.py\", line 567, in read\r\n",
        "    s = self.fp.read(amt)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/socket.py\", line 380, in read\r\n",
        "    data = self._sock.recv(left)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/ssl.py\", line 241, in recv\r\n",
        "    return self.read(buflen)\r\n",
        "  File \"/Users/erriza/anaconda/lib/python2.7/ssl.py\", line 160, in read\r\n",
        "    return self._sslobj.read(len)\r\n",
        "KeyboardInterrupt\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# need 20 lines\n",
      "!head -n1 output.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"delete\":{\"status\":{\"id\":331106645725806592,\"user_id\":484704907,\"id_str\":\"331106645725806592\",\"user_id_str\":\"484704907\"}}}\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 2: compute the sentiment of a tweet given known words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head AFINN-111.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abandon\t-2\r\n",
        "abandoned\t-2\r\n",
        "abandons\t-2\r\n",
        "abducted\t-2\r\n",
        "abduction\t-2\r\n",
        "abductions\t-2\r\n",
        "abhor\t-3\r\n",
        "abhorred\t-3\r\n",
        "abhorrent\t-3\r\n",
        "abhors\t-3\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build word:sentiment dictionary\n",
      "sentiment = {}\n",
      "with open('AFINN-111.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        term, score = line.split('\\t')\n",
      "        sentiment[term] = int(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to do some characterisations of the dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = [x.split() for x in sentiment]\n",
      "keylen = np.array([len(x) for x in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some entries consist of more than one words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array(words)[keylen > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([['cool', 'stuff'], ['green', 'wash'], ['messing', 'up'],\n",
        "       ['does', 'not', 'work'], [\"can't\", 'stand'], ['no', 'fun'],\n",
        "       ['not', 'working'], ['right', 'direction'], ['some', 'kind'],\n",
        "       ['cashing', 'in'], ['screwed', 'up'], ['dont', 'like'],\n",
        "       ['green', 'washing'], ['fed', 'up'], ['not', 'good']], dtype=object)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This complicates the matter. If a tweet contains the phrase \"...cool stuff...\", how should the sentiment be computed? based on \"cool\" only, \"cool stuff\" only, or the sum of both \"cool\" and \"cool stuff\"? \n",
      "\n",
      "The given definition of **tweet sentiment** is: **the sum of the sentiment scores for each term in the tweet**. The definition doesn't say anything on what to do if a tweet contains n-grams ($n \\geq 1$) which are all in the dictionary.\n",
      "\n",
      "The maximum phrase length in the dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max(keylen)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does any of the dictionary words/phrases contain some punctuation marks?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1 in [c in x for x in sentiment for c in '.,?!']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "\n",
      "string.punctuation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The actual computation of the tweet sentiment:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the mapping is used for unicode `translate` method\n",
      "remove_punctuation_map = dict((ord(char), None) for char in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: lower case, no punctuation\n",
      "            words = json.loads(line)['text'].translate(remove_punctuation_map).lower().strip().split()\n",
      "            \n",
      "            # chosen n-gram strategy: build all n-grams and consider all matches\n",
      "            # we know that there's only up to 3 words in the dict\n",
      "            temp = list(words)\n",
      "            for n in xrange(2, 4):\n",
      "                myNgrams = [temp[i:i+n] for i in xrange(len(temp) - n)]\n",
      "                for x in myNgrams:\n",
      "                    words.append(' '.join(x))\n",
      "\n",
      "            print float( sum([sentiment[x] if x in sentiment else 0 for x in words]) )\n",
      "            \n",
      "        except:\n",
      "            pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6.0\n",
        "-3.0\n",
        "0.0\n",
        "0.0\n",
        "4.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "0.0\n",
        "-2.0\n",
        "0.0\n",
        "3.0\n",
        "-3.0\n",
        "2.0\n",
        "-4.0\n",
        "0.0\n",
        "2.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 3: compute sentiment of unknown words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem specification is not so clear, i.e. not specific, about how exactly to assign score to unknown words. A simple approach is assign a score to every unknown word in a tweet equal to the sum of the sentiments of the tweets in which the unknown word appears. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "unknowns = defaultdict(int)\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: \n",
      "            # we don't consider n-grams here, only one-words\n",
      "            # the ascii encoding is due to a suggestion in the forum\n",
      "            words = json.loads(line)['text'].encode('ascii', 'ignore').strip().split()\n",
      "\n",
      "            score = 0\n",
      "            unk = []\n",
      "            for x in words:\n",
      "                if x in sentiment:\n",
      "                    score += sentiment[x]\n",
      "                else:\n",
      "                    unk.append(x)\n",
      "            \n",
      "            # the score of an unknown word is defined as:\n",
      "            # the sum of the tweet scores in which the word appears\n",
      "            for x in unk:\n",
      "                unknowns[x] += score\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "\n",
      "# !!!TAB SEPARATED\n",
      "for k, v in unknowns.iteritems():\n",
      "    print '%s\\t%.3f' % (k.encode('UTF-8'), float(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":)\t0.000\n",
        "L'EVOLUZIONE\t0.000\n",
        "http://t.co/\t0.000\n",
        "asleep\"\t4.000\n",
        "RT\t0.000\n",
        "1\t0.000\n",
        "20\t-1.000\n",
        "evolucin\t0.000\n",
        "seems\t2.000\n",
        ",\t0.000\n",
        "to\t4.000\n",
        "culo\"\t0.000\n",
        "depender\t0.000\n",
        "has\t-5.000\n",
        "@augustolorenzz:\t0.000\n",
        "porque\t0.000\n",
        "do\t0.000\n",
        "it,\t2.000\n",
        "read\t4.000\n",
        "de\t0.000\n",
        "YEAH\t0.000\n",
        "#italia\t0.000\n",
        "it!\t2.000\n",
        "SHREDS.\t0.000\n",
        "fat,\t-4.000\n",
        "@AndyF2K10\t2.000\n",
        "maquina\t2.000\n",
        "increble\t0.000\n",
        "Global\t0.000\n",
        "\"did\t0.000\n",
        "foi\t0.000\n",
        ":'(\t0.000\n",
        "desde\t0.000\n",
        "kkkkkkkk'\t0.000\n",
        "NIS\t0.000\n",
        "@Latingable1D:\t0.000\n",
        "COMEMORA\t0.000\n",
        "everyone\t2.000\n",
        "@myidolovesme:\t0.000\n",
        "HAWKS\t0.000\n",
        "DEI\t0.000\n",
        "are\t0.000\n",
        "Mama\t-4.000\n",
        "HELL\t0.000\n",
        "es\t2.000\n",
        "really\t0.000\n",
        "#\t0.000\n",
        "opened\t-1.000\n",
        "COM\t0.000\n",
        "hooked\t2.000\n",
        "#comida\t0.000\n",
        "@wamalikk98\t0.000\n",
        "@__claraaa\t2.000\n",
        "WORD!!!\t0.000\n",
        "be\t4.000\n",
        "perfezione.\"\t0.000\n",
        "Mi\t2.000\n",
        "http://t.co/WILzGKWvsw\t0.000\n",
        "cara\t0.000\n",
        "ja\t0.000\n",
        "lol\"\t0.000\n",
        "mejorado\t2.000\n",
        "#ParanormalActivity4\t-1.000\n",
        "of\t0.000\n",
        "la\t2.000\n",
        "DM.\t0.000\n",
        "una\t2.000\n",
        "tour\t0.000\n",
        "3.\t0.000\n",
        "let\t0.000\n",
        "@DrSoxion:\t0.000\n",
        "personajes\t0.000\n",
        "Es\t0.000\n",
        "own\t-4.000\n",
        "itself.\t-1.000\n",
        "number\t0.000\n",
        "Lol\t0.000\n",
        "mucho\t2.000\n",
        "warped\t0.000\n",
        "@DoniyaElisha\t0.000\n",
        "message\t4.000\n",
        "@Aaroosa_M\t0.000\n",
        "http://t.co/5u6rcA5aQW\t0.000\n",
        "\"fell\t4.000\n",
        "\"sei\t0.000\n",
        "gay.\t0.000\n",
        "her\t-4.000\n",
        "sup?\t0.000\n",
        "it's\t-4.000\n",
        "by\t-1.000\n",
        "Grandma)\t0.000\n",
        "Disappointed.\t-1.000\n",
        "slowly\t-1.000\n",
        "baby.\t0.000\n",
        "pic\t0.000\n",
        "door\t-1.000\n",
        "F\t0.000\n",
        "you?\t0.000\n",
        "hi\t0.000\n",
        "Aee\t0.000\n",
        "timinho\t0.000\n",
        "\"ti\t0.000\n",
        "must\t2.000\n",
        "me\t2.000\n",
        "this\t0.000\n",
        "churri\t2.000\n",
        "mi\t0.000\n",
        "congressman.\t-4.000\n",
        "my\t4.000\n",
        "and\t-1.000\n",
        "puzza\t0.000\n",
        "minutes\t-1.000\n",
        "is\t-4.000\n",
        "ano\t0.000\n",
        "it\t2.000\n",
        "all'inizio:\t0.000\n",
        "il\t0.000\n",
        "in\t-1.000\n",
        "@Fizzahx\t0.000\n",
        "ora:\t0.000\n",
        "@gooodapollo:\t0.000\n",
        "Yo\t-4.000\n",
        "Food\t0.000\n",
        "fan..\t0.000\n",
        "when\t4.000\n",
        "#likeforlike\t0.000\n",
        "@Arlenysluna\t0.000\n",
        "how\t4.000\n",
        "que\t2.000\n",
        "you\t8.000\n",
        "@ajjenkins\t0.000\n",
        "Man\t0.000\n",
        ":):)\t2.000\n",
        "noche\t2.000\n",
        "fatty\t-2.000\n",
        "I\t2.000\n",
        "primeira\t0.000\n",
        "Direction's\t0.000\n",
        "@2fa_Malik\t0.000\n",
        "@MeatSauce1:\t0.000\n",
        "Iron\t0.000\n",
        "YOU\t0.000\n",
        "on\t2.000\n",
        "@14Saik14\t0.000\n",
        "a\t0.000\n",
        "PARISE\t0.000\n",
        "#lasagna\t0.000\n",
        "m\t0.000\n",
        "One\t0.000\n",
        "hasta\t0.000\n",
        "@MaryumMalik_:\t0.000\n",
        "dopo:\t0.000\n",
        "so\t-4.000\n",
        "talk\t2.000\n",
        "vocs\t0.000\n",
        ",,\t0.000\n",
        "se\t0.000\n",
        "(Zayn's\t0.000\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 4: compute term frequency"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordict = defaultdict(int)\n",
      "\n",
      "with open('output.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            # chosen normalisation: don't consider n-grams\n",
      "            words = json.loads(line)['text'].strip().split()\n",
      "\n",
      "            for x in words:\n",
      "                wordict[x] += 1\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "\n",
      "denom = sum(wordict.values())\n",
      "\n",
      "for k, v in wordict.iteritems():\n",
      "    print '%s\\t%.4f' % (k.encode('UTF-8'), float(v) / denom)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ":)\t0.0054\n",
        "L'EVOLUZIONE\t0.0054\n",
        "evoluci\u00f3n\t0.0054\n",
        "fat,\t0.0054\n",
        "asleep\"\t0.0054\n",
        "RT\t0.0380\n",
        "1\t0.0109\n",
        "ha\t0.0054\n",
        "20\t0.0054\n",
        "how\t0.0054\n",
        "seems\t0.0054\n",
        ",\t0.0054\n",
        "to\t0.0109\n",
        "culo\"\t0.0054\n",
        "depender\t0.0054\n",
        "has\t0.0109\n",
        "worth\t0.0054\n",
        "@augustolorenzz:\t0.0054\n",
        "porque\t0.0054\n",
        "do\t0.0109\n",
        "it,\t0.0054\n",
        "read\t0.0054\n",
        "de\t0.0109\n",
        "YEAH\t0.0054\n",
        "#italia\t0.0054\n",
        "it!\t0.0054\n",
        "SHREDS.\t0.0054\n",
        "@AndyF2K10\t0.0054\n",
        "maquina\t0.0054\n",
        "Global\t0.0054\n",
        "\"did\t0.0054\n",
        "foi\t0.0054\n",
        ":'(\t0.0054\n",
        "desde\t0.0054\n",
        "kkkkkkkk'\t0.0054\n",
        "@Latingable1D:\t0.0054\n",
        "COMEMORA\t0.0054\n",
        "ass\t0.0054\n",
        "everyone\t0.0054\n",
        "@myidolovesme:\t0.0054\n",
        "HAWKS\t0.0054\n",
        "DEI\t0.0054\n",
        "are\t0.0054\n",
        "Mama\t0.0054\n",
        "HELL\t0.0054\n",
        "es\t0.0054\n",
        "really\t0.0054\n",
        "#\t0.0054\n",
        "opened\t0.0054\n",
        "COM\t0.0054\n",
        "hooked\t0.0054\n",
        "#comida\t0.0054\n",
        "noche\t0.0054\n",
        "@wamalikk98\t0.0054\n",
        "@__claraaa\t0.0054\n",
        "WORD!!!\t0.0054\n",
        "be\t0.0109\n",
        "perfezione.\"\t0.0054\n",
        "incre\u00edble\t0.0054\n",
        "Mi\t0.0054\n",
        "m\u2026\t0.0054\n",
        "http://t.co/WILzGKWvsw\t0.0054\n",
        "cara\t0.0054\n",
        "ja\t0.0054\n",
        "lol\"\t0.0054\n",
        "http://t.co/\u2026\t0.0054\n",
        "mejorado\t0.0054\n",
        "#ParanormalActivity4\t0.0054\n",
        "miss\t0.0054\n",
        "of\t0.0054\n",
        "la\t0.0217\n",
        "DM.\t0.0054\n",
        "una\t0.0054\n",
        "tour\t0.0054\n",
        "3.\t0.0054\n",
        "let\t0.0054\n",
        "@DrSoxion:\t0.0054\n",
        "personajes\t0.0054\n",
        "Es\t0.0054\n",
        "own\t0.0054\n",
        "itself.\t0.0054\n",
        "number\t0.0054\n",
        "Lol\t0.0054\n",
        "mucho\t0.0054\n",
        "warped\t0.0054\n",
        "@DoniyaElisha\t0.0054\n",
        "message\t0.0054\n",
        "@Aaroosa_M\t0.0054\n",
        "http://t.co/5u6rcA5aQW\t0.0054\n",
        "\"fell\t0.0054\n",
        "\"sei\t0.0054\n",
        "gay.\t0.0054\n",
        "her\t0.0054\n",
        "sup?\t0.0054\n",
        "it's\t0.0054\n",
        "by\t0.0054\n",
        "Grandma)\t0.0054\n",
        "Disappointed.\t0.0054\n",
        "slowly\t0.0054\n",
        "baby.\t0.0054\n",
        "pic\t0.0054\n",
        "door\t0.0054\n",
        "F\t0.0109\n",
        "you?\t0.0054\n",
        "hi\t0.0054\n",
        "Aee\t0.0054\n",
        "timinho\t0.0054\n",
        "\"ti\t0.0054\n",
        "must\t0.0054\n",
        "me\t0.0054\n",
        "this\t0.0054\n",
        "churri\t0.0054\n",
        "mi\t0.0054\n",
        "congressman.\t0.0054\n",
        "\ud83d\ude0f\ud83d\udc4d\t0.0054\n",
        "my\t0.0054\n",
        "and\t0.0109\n",
        "puzza\t0.0054\n",
        "minutes\t0.0054\n",
        "is\t0.0054\n",
        "ano\t0.0054\n",
        "it\t0.0054\n",
        "all'inizio:\t0.0054\n",
        "il\t0.0054\n",
        "in\t0.0054\n",
        "@Fizzahx\t0.0054\n",
        "ora:\t0.0054\n",
        "@gooodapollo:\t0.0054\n",
        "funny\t0.0054\n",
        "Yo\t0.0054\n",
        "no\t0.0054\n",
        "Food\t0.0054\n",
        "fan..\t0.0054\n",
        "when\t0.0054\n",
        "#likeforlike\t0.0054\n",
        "@Arlenysluna\t0.0054\n",
        "N\u00d3IS\t0.0054\n",
        "que\t0.0054\n",
        "you\t0.0163\n",
        "@ajjenkins\t0.0054\n",
        "Man\t0.0054\n",
        ":):)\t0.0054\n",
        "voc\u00eas\t0.0054\n",
        "fatty\t0.0054\n",
        "I\t0.0054\n",
        "primeira\t0.0054\n",
        "Direction's\t0.0054\n",
        "@2fa_Malik\t0.0054\n",
        "@MeatSauce1:\t0.0054\n",
        "Iron\t0.0054\n",
        "YOU\t0.0054\n",
        "on\t0.0054\n",
        "@14Saik14\t0.0054\n",
        "a\t0.0054\n",
        "PARISE\t0.0054\n",
        "#lasagna\t0.0054\n",
        "One\t0.0054\n",
        "hasta\t0.0054\n",
        "@MaryumMalik_:\t0.0054\n",
        "dopo:\t0.0054\n",
        "so\t0.0054\n",
        "talk\t0.0054\n",
        ",,\t0.0054\n",
        "se\t0.0054\n",
        "(Zayn's\t0.0054\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 5: happiest state"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the mapping is used for unicode `translate` method\n",
      "remove_punctuation_map = dict((ord(char), None) for char in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
      "\n",
      "# data structure: state : [count, sum]\n",
      "states = {x : [0, 0] for x in [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
      "                               \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
      "                               \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
      "                               \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
      "                               \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]}\n",
      "\n",
      "with open('output_20130505_2351.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "            \n",
      "            country = tweet['place']['country']\n",
      "            state = tweet['place']['full_name'][-2::]\n",
      "            \n",
      "            if country=='United States' and state in states:\n",
      "                # chosen normalisation: lower case, no punctuation, don't consider n-grams\n",
      "                words = tweet['text'].translate(remove_punctuation_map).lower().strip().split()\n",
      "                sent = float( sum([sentiment[x] if x in sentiment else 0 for x in words]) )\n",
      "            \n",
      "                states[state][0] += 1\n",
      "                states[state][1] += sent\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "\n",
      "# compute the value that should be used for ranking\n",
      "def get_rank(v):\n",
      "    if states[v][0]:\n",
      "        return float(states[v][1]) / states[v][0]\n",
      "    else:\n",
      "        # if there's no tweet from the state, give large negative\n",
      "        return -1000\n",
      "    \n",
      "print str(max(states, key=get_rank)).encode('UTF-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NV\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Problem 6: top ten hashtags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordict = defaultdict(int)\n",
      "\n",
      "with open('output_20130505_2351.txt', 'r') as json_data:\n",
      "    for line in json_data:\n",
      "        try:\n",
      "            tweet = json.loads(line)\n",
      "            \n",
      "            for ht in tweet['entities']['hashtags']:\n",
      "                word = ht['text']\n",
      "                \n",
      "                wordict[word] += 1\n",
      "            \n",
      "        except:\n",
      "            pass\n",
      "        \n",
      "top_ten = sorted(wordict, key=lambda x: wordict[x], reverse=True)[:10]\n",
      "\n",
      "for i in top_ten:\n",
      "    print '%s\\t%.4f' % (i.encode('UTF-8'), float(wordict[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT\t19.0000\n",
        "gameinsight\t15.0000\n",
        "LifeWouldBeAlotBetterIf\t8.0000\n",
        "Nicol\u00e1sT\u00faNoGanaste\t7.0000\n",
        "ipad\t7.0000\n",
        "A2MesesDeTuPartidaChavez\t7.0000\n",
        "ipadgames\t7.0000\n",
        "QueCadaCaprilistaTengaUnMillonDeSeguidores\t7.0000\n",
        "androidgames\t6.0000\n",
        "veracruz\t6.0000\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}